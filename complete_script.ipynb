{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "complete_model_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python37464bite82ff674e76f4e35b0c7582fef46bd24",
      "display_name": "Python 3.7.4 64-bit"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "colab_type": "code",
        "id": "5Z-O16QDJqpV",
        "outputId": "dadecb4d-4f71-4fb7-ab45-a504f36c2307"
      },
      "outputs": [],
      "source": [
        "# COLAB ONLY STUFF\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kOPdgThGlhf3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, filepath):\n",
        "        cwd = os.getcwd()\n",
        "        self.basepath = filepath\n",
        "        try:\n",
        "            os.stat(self.basepath+\"/add_prim_split\")\n",
        "            os.stat(self.basepath+\"/few_shot_split\")\n",
        "            os.stat(self.basepath+\"/filler_split\")\n",
        "            os.stat(self.basepath+\"/length_split\")\n",
        "            os.stat(self.basepath+\"/simple_split\")\n",
        "            os.stat(self.basepath+\"/template_split\")\n",
        "        except Exception as e:\n",
        "            raise Exception(\"Path \"+filepath+\" doesnt seem to contain the required folders.\")\n",
        "\n",
        "    def load_1a(self):\n",
        "        train = self.file_loader(\"/simple_split/tasks_train_simple.txt\")\n",
        "        test = self.file_loader(\"/simple_split/tasks_test_simple.txt\")\n",
        "\n",
        "        return (np.asarray(train), np.asarray(test))\n",
        "\n",
        "    def load_1b(self):\n",
        "        percentile_dict = {}\n",
        "        splits = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\"]\n",
        "\n",
        "        for percentile in splits:\n",
        "            train = self.file_loader(\"/simple_split/size_variations/tasks_train_simple_p{}.txt\".format(percentile))\n",
        "            test = self.file_loader(\"/simple_split/size_variations/tasks_test_simple_p{}.txt\".format(percentile))\n",
        "            \n",
        "            percentile_dict[percentile] = (np.asarray(train), np.asarray(test))\n",
        "            \n",
        "        return percentile_dict\n",
        "\n",
        "    def load_2(self):\n",
        "        train = self.file_loader(\"/length_split/tasks_train_length.txt\")\n",
        "        test = self.file_loader(\"/length_split/tasks_test_length.txt\")\n",
        "\n",
        "        return (np.asarray(train), np.asarray(test))\n",
        "\n",
        "    def load_3(self):\n",
        "        \"\"\"\n",
        "        loads the datasets for both parts of the experiment\n",
        "        the first part where both primitives appear without compositional commands\n",
        "        the second part where 'jump' primitive appears in\n",
        "        compositional commands of varying lengths\n",
        "        returns a dictionary of pairs all possible train/test sets\n",
        "        \"\"\"\n",
        "        data_dict = {}\n",
        "        nums = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]\n",
        "        reps = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
        "\n",
        "        train = self.file_loader(\"/add_prim_split/tasks_train_addprim_jump.txt\")\n",
        "        test = self.file_loader(\"/add_prim_split/tasks_test_addprim_jump.txt\")\n",
        "        data_dict['jump'] = (np.asarray(train), np.asarray(test))\n",
        "\n",
        "        train = self.file_loader(\"/add_prim_split/tasks_train_addprim_turn_left.txt\")\n",
        "        test = self.file_loader(\"/add_prim_split/tasks_test_addprim_turn_left.txt\")\n",
        "        data_dict['lturn'] = (np.asarray(train), np.asarray(test))\n",
        "        \n",
        "        for num in nums:\n",
        "            for rep in reps:\n",
        "                train = self.file_loader(\"/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num{}_rep{}.txt\".format(num, rep))\n",
        "                test = self.file_loader(\"/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num{}_rep{}.txt\".format(num, rep))\n",
        "                \n",
        "                data_dict['jump_num{}_rep{}'.format(num, rep)] = (np.asarray(train), np.asarray(test))\n",
        "            \n",
        "        return data_dict\n",
        "\n",
        "    def file_loader(self, path):\n",
        "        sent_list = []\n",
        "        with open(self.basepath+path, \"r\") as f:\n",
        "                    for line in f:\n",
        "                        sent_list.append(line_splitter(line))\n",
        "        return sent_list\n",
        "\n",
        "    \n",
        "def line_splitter(sentence):\n",
        "    sent_list = sentence.split(\"OUT: \")\n",
        "    sent_list[0] = sent_list[0].strip(\"IN: \")\n",
        "    sent_list[1] = sent_list[1].strip(\"\\n\")\n",
        "\n",
        "    return sent_list\n",
        "\n",
        "# examples:\n",
        "# 1a :\n",
        "#   train, test = dl.load_1a()\n",
        "#   train[0][0] first train sentence, \"IN\"\n",
        "#   train[0][1] first train sentence, \"OUT\"\n",
        "# 1b :\n",
        "#   dict = dl.load_1b()\n",
        "#   train, test = dict[\"1\"] extract the 1 percentile sentences out, split into train and test\n",
        "#   train[0][0] first train sentence, \"OUT\"\n",
        "#   train[0][1] first train sentence, \"OUT\"\n",
        "#\n",
        "# all returns are numpy arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FjVZyIyUlhgC"
      },
      "outputs": [],
      "source": [
        "#from data_loader import DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Input:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "class Output:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "        \n",
        "def get_embedding(word, lookup_dict, embeds):\n",
        "    tensor = torch.tensor([lookup_dict[word]], dtype=torch.long)\n",
        "    return embeds(tensor)\n",
        "\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    output_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, output_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pvsGAjCVlhgM"
      },
      "outputs": [],
      "source": [
        "#from data_loader import *\n",
        "#from embeddings import *\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.0, layers=1, mode='RNN'):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.layers = layers\n",
        "        self.mode = mode\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        self.hidden_layer = nn.RNN(self.hidden_size, self.hidden_size, num_layers=self.layers, dropout=self.dropout_p)\n",
        "\n",
        "        if self.mode == 'LSTM':\n",
        "        \tself.hidden_layer = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=self.layers, dropout=self.dropout_p)\n",
        "        elif self.mode == 'GRU':\n",
        "        \tself.hidden_layer = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.layers, dropout=self.dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output, hidden = self.hidden_layer(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        hidden = torch.zeros(self.layers, 1, self.hidden_size, device=device)\n",
        "        nn.init.xavier_uniform_(hidden, gain=nn.init.calculate_gain('relu'))\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.0, layers=1, attention=False, mode='RNN'):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.layers = layers\n",
        "        self.max_length = max_length\n",
        "        self.attention = attention\n",
        "        self.mode = mode\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        if self.attention:\n",
        "\t        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "\t        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "        self.hidden_layer = nn.RNN(self.hidden_size, self.hidden_size, num_layers=self.layers, dropout=self.dropout_p)\n",
        "\n",
        "        if self.mode == 'LSTM':\n",
        "        \tself.hidden_layer = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=self.layers, dropout=self.dropout_p)\n",
        "        elif self.mode == 'GRU':\n",
        "        \tself.hidden_layer = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.layers, dropout=self.dropout_p)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs=None):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        if self.attention:\n",
        "          if self.mode==\"LSTM\":\n",
        "            attn_weights = F.softmax(self.attn(torch.cat((output[0], hidden[0][0]), 1)), dim=1) #should be hidden[0][self.layers]\n",
        "          else:\n",
        "            attn_weights = F.softmax(self.attn(torch.cat((output[0], hidden[0]), 1)), dim=1) #should be hidden[self.layers]\n",
        "          attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
        "          output = torch.cat((output[0], attn_applied[0]), 1)\n",
        "          output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.hidden_layer(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        hidden = torch.zeros(self.layers, 1, self.hidden_size, device=device)\n",
        "        nn.init.xavier_uniform_(hidden, gain=nn.init.calculate_gain('relu'))\n",
        "        return hidden\n",
        "\n",
        "def train(input_tensor, output_tensor, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion, max_length, clipping_value=5):\n",
        "    encoder_hidden1 = encoder.initHidden()\n",
        "    encoder_hidden2 = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    output_length = output_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        if encoder.mode == 'LSTM':\n",
        "            encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei], (encoder_hidden1, encoder_hidden2))\n",
        "        else: \n",
        "            encoder_output, encoder_hidden1 = encoder(input_tensor[ei], encoder_hidden1)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "    \n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden1 = encoder_hidden1\n",
        "    decoder_hidden2 = encoder_hidden2\n",
        "\n",
        "    forcing = random.random() > 0.5\n",
        "\n",
        "    if forcing:\n",
        "        for di in range(output_length):\n",
        "            if decoder.mode == 'LSTM':\n",
        "              decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "            else:\n",
        "              decoder_output, decoder_hidden1 = decoder(decoder_input, decoder_hidden1, encoder_outputs)\n",
        "            \n",
        "            decoder_input = output_tensor[di]\n",
        "            loss += criterion(decoder_output, output_tensor[di])\n",
        "\n",
        "            if decoder_input.item() == EOS_token:\n",
        "              break\n",
        "    else:\n",
        "        for di in range(output_length):\n",
        "            if decoder.mode == 'LSTM':\n",
        "              decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "            else:\n",
        "              decoder_output, decoder_hidden1 = decoder(decoder_input, decoder_hidden1, encoder_outputs)\n",
        "            \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            loss += criterion(decoder_output, output_tensor[di])\n",
        "            \n",
        "            if decoder_input.item() == EOS_token:\n",
        "              break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clipping_value)\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clipping_value)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item() / output_length\n",
        "\n",
        "    \n",
        "def trainIters(encoder, decoder, train_data, input_lang, output_lang, max_length, learning_rate=0.001):\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    losses = []\n",
        "    print(train_data.shape[0])\n",
        "    print_loss_total = 0\n",
        "\n",
        "    for iter in range(train_data.shape[0]):\n",
        "        training_pair = tensorsFromPair(train_data[iter], input_lang, output_lang)\n",
        "        input_tensor = training_pair[0]\n",
        "        output_tensor = training_pair[1]\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            input_tensor = input_tensor.cuda()\n",
        "            output_tensor = output_tensor.cuda()\n",
        "        \n",
        "        loss = train(input_tensor, output_tensor, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion, max_length)\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % 1000 == 0:\n",
        "            print_loss_avg = print_loss_total / 500\n",
        "            losses.append(print_loss_avg)\n",
        "            print(iter)\n",
        "            print(print_loss_avg)\n",
        "            print_loss_total = 0\n",
        "\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5ZlSSM1JlhgW"
      },
      "outputs": [],
      "source": [
        "def train_and_save(train_data, train_in, train_out, model, dropout, att, layers, model_name, file_prefix, hidden_units=200, MAX_LENGTH=100):\n",
        "    encoder = Encoder(train_in.n_words, hidden_units, layers=layers, mode=model, dropout_p=dropout)\n",
        "    decoder = Decoder(hidden_units, train_out.n_words, layers=layers, max_length=MAX_LENGTH, mode=model, dropout_p=dropout, attention=att)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "\n",
        "    losses = trainIters(encoder, decoder, train_data, train_in, train_out, MAX_LENGTH)\n",
        "    plt.plot(losses)\n",
        "    plt.title(model+'_layers='+str(layers)+'_drop='+str(dropout)+'_attention='+str(att))\n",
        "    plt.xlabel('iterations')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show()\n",
        "    torch.save(encoder.state_dict(), file_prefix+model_name+\"_encoder.pt\")\n",
        "    torch.save(decoder.state_dict(), file_prefix+model_name+\"_decoder.pt\")\n",
        "\n",
        "def load_models(train_in_nwords, train_out_nwords, hidden_size, layers, mode, dropout_p, attention, file_location, model_name, max_length=100):\n",
        "    encoder = Encoder(train_in_nwords, hidden_size, layers=layers, mode=mode, dropout_p=dropout_p)\n",
        "    encoder.load_state_dict(torch.load(file_location+model_name+\"_encoder.pt\"))\n",
        "    encoder.eval()\n",
        "\n",
        "    decoder = Decoder(hidden_size, train_out_nwords, max_length, layers=layers, mode=mode, dropout_p=dropout_p, attention=attention)\n",
        "    decoder.load_state_dict(torch.load(file_location+model_name+\"_decoder.pt\"))\n",
        "    decoder.eval()\n",
        "\n",
        "    return encoder, decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OMnLV4Yllhgh"
      },
      "outputs": [],
      "source": [
        "#from data_loader import *\n",
        "#from embeddings import *\n",
        "#from layers_attempt import *\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, train_in, train_out, max_length=100):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = tensorFromSentence(train_in, sentence)\n",
        "    if torch.cuda.is_available():\n",
        "      input_tensor = input_tensor.cuda()\n",
        "      encoder.cuda()\n",
        "      decoder.cuda()\n",
        "\n",
        "    input_length = input_tensor.size()[0]\n",
        "    encoder_hidden1 = torch.zeros(encoder.layers, 1, encoder.hidden_size, device=device)\n",
        "    encoder_hidden2 = torch.zeros(encoder.layers, 1, encoder.hidden_size, device=device)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      if encoder.mode == \"LSTM\":\n",
        "        encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei],(encoder_hidden1, encoder_hidden2))\n",
        "      else:\n",
        "        encoder_output, encoder_hidden1 = encoder(input_tensor[ei], encoder_hidden1)\n",
        "      encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden1 = encoder_hidden1\n",
        "    decoder_hidden2 = encoder_hidden2\n",
        "\n",
        "    decoded_words = []\n",
        "    \n",
        "    for di in range(max_length):\n",
        "      if decoder.mode == \"LSTM\":\n",
        "        decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "      else:\n",
        "        decoder_output, decoder_hidden1 = decoder(decoder_input, decoder_hidden1, encoder_outputs)\n",
        "      \n",
        "      topv, topi = decoder_output.data.topk(1) \n",
        "      \n",
        "      if topi.item() == EOS_token:\n",
        "        break\n",
        "      else:\n",
        "        decoded_words.append(train_out.index2word[topi.item()])\n",
        "      \n",
        "      decoder_input = topi.squeeze().detach()\n",
        "\n",
        "    return decoded_words\n",
        "\n",
        "def evaluateIters(test_data, encoder, decoder, train_in, train_out):\n",
        "    hit = 0\n",
        "    miss = 0\n",
        "    iters = 0\n",
        "    hit_idx = []\n",
        "    miss_idx = []\n",
        "\n",
        "    for idx, test_point in enumerate(test_data):\n",
        "        pred_list = evaluate(encoder, decoder, test_point[0], train_in, train_out)\n",
        "        pred = \" \".join(pred_list)\n",
        "        if pred == test_point[1]:\n",
        "            hit += 1\n",
        "            hit_idx.append(idx)\n",
        "        else:\n",
        "            miss += 1\n",
        "            miss_idx.append(idx)\n",
        "        iters += 1\n",
        "\n",
        "        if iters % 100 == 0:\n",
        "            print(iters)\n",
        "            print(hit)\n",
        "\n",
        "    return hit, hit_idx, miss, miss_idx\n",
        "\n",
        "def evaluate_and_save(test_data, model_name, save_file, encoder, decoder, train_in, train_out):\n",
        "    print(encoder.hidden_size)\n",
        "    hit, hit_idx, miss, miss_idx = evaluateIters(test_data, encoder, decoder, train_in, train_out)\n",
        "    acc = 1-miss/len(test_data)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/models/\"+save_file, 'a') as f:\n",
        "        f.write(\"Model name: \" + model_name + \"\\n\")\n",
        "        f.write(\"Hits: \" + str(hit) + \"\\n\")\n",
        "        f.write(\"Miss: \" + str(miss) + \"\\n\")\n",
        "        f.write(\"Accuracy: \" + str(acc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-FmPKRDkGFNL"
      },
      "outputs": [],
      "source": [
        "##### ALTERNATIVE EVALUATION #####\n",
        "\n",
        "#from data_loader import *\n",
        "#from embeddings import *\n",
        "#from layers_attempt import *\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def evaluateAlt(encoder, decoder, sentence, train_in, train_out, vectors=False, actual_length=0, max_length=100):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = tensorFromSentence(train_in, sentence)\n",
        "    if torch.cuda.is_available():\n",
        "      input_tensor = input_tensor.cuda()\n",
        "      encoder.cuda()\n",
        "      decoder.cuda()\n",
        "\n",
        "    input_length = input_tensor.size()[0]\n",
        "    encoder_hidden1 = torch.zeros(encoder.layers, 1, encoder.hidden_size, device=device)\n",
        "    encoder_hidden2 = torch.zeros(encoder.layers, 1, encoder.hidden_size, device=device)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      if encoder.mode == \"LSTM\":\n",
        "        encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei],(encoder_hidden1, encoder_hidden2))\n",
        "      else:\n",
        "        encoder_output, encoder_hidden1 = encoder(input_tensor[ei], encoder_hidden1)\n",
        "      encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    hidden_state_vector = encoder_hidden1\n",
        "    decoder_hidden1 = encoder_hidden1\n",
        "    decoder_hidden2 = encoder_hidden2\n",
        "\n",
        "    decoded_words = []\n",
        "    \n",
        "    for di in range(max_length):\n",
        "      if decoder.mode == \"LSTM\":\n",
        "        decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "      else:\n",
        "        decoder_output, decoder_hidden1 = decoder(decoder_input, decoder_hidden1, encoder_outputs)\n",
        "      \n",
        "      topv, topi = decoder_output.data.topk(2)\n",
        "            \n",
        "      if topi[0][0].item() == EOS_token:\n",
        "        if actual_length != 0:\n",
        "          if len(decoded_words) == actual_length:\n",
        "            break\n",
        "          else:\n",
        "            decoded_words.append(train_out.index2word[topi[0][1].item()])\n",
        "        else:\n",
        "          break\n",
        "      elif topi[0][0].item() != EOS_token:\n",
        "        decoded_words.append(train_out.index2word[topi[0][0].item()])\n",
        "      \n",
        "      topv, topi = decoder_output.data.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "\n",
        "    if vectors:\n",
        "      return decoded_words, hidden_state_vector\n",
        "    else:\n",
        "      return decoded_words\n",
        "\n",
        "def evaluateItersAlt(test_data, encoder, decoder, train_in, train_out, oracle=False, vectors=False):\n",
        "    hit = 0\n",
        "    miss = 0\n",
        "    iters = 0\n",
        "    hit_idx = []\n",
        "    miss_idx = []\n",
        "    hidden_state_vectors = []\n",
        "\n",
        "    for idx, test_point in enumerate(test_data):\n",
        "        if oracle:\n",
        "          pred_list = evaluateAlt(encoder, decoder, test_point[0], train_in, train_out, actual_length=len(test_point[1].split()))\n",
        "        elif vectors:\n",
        "          pred_list, hidden_state_vector = evaluateAlt(encoder, decoder, test_point[0], train_in, train_out, vectors=vectors)\n",
        "          hidden_state_vectors.append((test_point[0],hidden_state_vector))\n",
        "        else: \n",
        "          pred_list = evaluateAlt(encoder, decoder, test_point[0], train_in, train_out)\n",
        "        pred = \" \".join(pred_list)\n",
        "        if pred == test_point[1]:\n",
        "            hit += 1\n",
        "            hit_idx.append(idx)\n",
        "        else:\n",
        "            miss += 1\n",
        "            miss_idx.append(idx)\n",
        "        iters += 1\n",
        "\n",
        "        if iters % 500 == 0:\n",
        "            print(iters)\n",
        "            #print(hit)\n",
        "    if vectors:\n",
        "      return hit, hit_idx, miss, miss_idx, hidden_state_vectors\n",
        "    else:\n",
        "      return hit, hit_idx, miss, miss_idx\n",
        "\n",
        "def evaluate_and_save(test_data, model_name, save_file, encoder, decoder, train_in, train_out):\n",
        "    print(encoder.hidden_size)\n",
        "    hit, hit_idx, miss, miss_idx, hidden_state_vector = evaluateIters(test_data, encoder, decoder, train_in, train_out)\n",
        "    acc = 1-miss/len(test_data)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/models/\"+save_file, 'a') as f:\n",
        "        f.write(\"Model name: \" + model_name + \"\\n\")\n",
        "        f.write(\"Hits: \" + str(hit) + \"\\n\")\n",
        "        f.write(\"Miss: \" + str(miss) + \"\\n\")\n",
        "        f.write(\"Accuracy: \" + str(acc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "colab_type": "code",
        "id": "Iu8aANmvlhgy",
        "outputId": "50635455-ed4c-48ca-ead7-926088528a28"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#USED IN COLAB\n",
        "dl = DataLoader(\"/content/drive/My Drive/Colab Notebooks/SCAN\")\n",
        "\n",
        "#USED ON OWN PC\n",
        "#dl = DataLoader(\"SCAN\")\n",
        "\n",
        "#MAX_LENGTH = max([len(x[0].split()) for x in train_data]) + 1\n",
        "MAX_LENGTH = 100\n",
        "\n",
        "#DATA LOADING AND LANGUAGE CREATION, DIFFERS BETWEEN EXERCISES\n",
        "data_dict = dl.load_3()\n",
        "\n",
        "train_in = Input(\"train_input\")\n",
        "train_out = Output(\"train_output\")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "\n",
        "\n",
        "file_location = \"/content/drive/My Drive/Colab Notebooks/models/\"\n",
        "hidden_units=100\n",
        "model='GRU'\n",
        "model_name = \"3rd_GRU_100_dim_1_layer_att_0.1_drop_2_percent_split\"\n",
        "\n",
        "for i in range(1, 6):\n",
        "    train_data, test_data = data_dict[\"jump_num2_rep\"+str(i)]\n",
        "\n",
        "    for datapoint in train_data:\n",
        "      train_in.addSentence(datapoint[0])\n",
        "      train_out.addSentence(datapoint[1])\n",
        "\n",
        "    test_in = Input(\"test_input\")\n",
        "    test_out = Output(\"test_output\")\n",
        "\n",
        "    for datapoint in test_data:\n",
        "      test_in.addSentence(datapoint[0])\n",
        "      test_out.addSentence(datapoint[1])\n",
        "\n",
        "    actual_train_data = train_data[np.random.choice(train_data.shape[0], 100000, replace=True), :]\n",
        "\n",
        "    train_and_save(actual_train_data, train_in, train_out, model, 0.1, True, 1, model_name+str(i), file_location, hidden_units=hidden_units)\n",
        "    encoder, decoder = load_models(train_in.n_words, train_out.n_words, hidden_units, 1, model, 0.1, True, file_location, model_name+str(i))\n",
        "    evaluate_and_save(test_data, model_name+str(i), model_name+str(i)+\".txt\", encoder, decoder, train_in, train_out)\n",
        "    print(\"################### ITERATION \" +str(i) +\" DONE ###################\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DO03sVtTATB-"
      },
      "outputs": [],
      "source": [
        "# Oracle\n",
        "dl = DataLoader(\"/content/drive/My Drive/Colab Notebooks/SCAN\")\n",
        "file_location = \"/content/drive/My Drive/Colab Notebooks/models/\"\n",
        "hidden_units=50\n",
        "model='GRU'\n",
        "\n",
        "score_dict = {}\n",
        "train_data, test_data = dl.load_2()\n",
        "\n",
        "train_in = Input(\"train_input\")\n",
        "train_out = Output(\"train_output\")\n",
        "\n",
        "test_in = Input(\"test_input\")\n",
        "test_out = Output(\"test_output\")\n",
        "\n",
        "\n",
        "for datapoint in train_data:\n",
        "    train_in.addSentence(datapoint[0])\n",
        "    train_out.addSentence(datapoint[1])\n",
        "\n",
        "for datapoint in test_data:\n",
        "    test_in.addSentence(datapoint[0])\n",
        "    test_out.addSentence(datapoint[1])\n",
        "\n",
        "for i in range(1, 6):\n",
        "  encoder, decoder = load_models(train_in.n_words, train_out.n_words, 50, 1, \"GRU\", 0.5, True, file_location, \"2nd_GRU_50_dim_1_layer_att_drop\"+str(i))\n",
        "  hit, hit_idx, miss, miss_idx = evaluateItersAlt(test_data, encoder, decoder, train_in, train_out, oracle=True)\n",
        "\n",
        "  score_dict[i] = (hit, hit_idx, miss, miss_idx, 1-miss/test_data.shape[0])\n",
        "\n",
        "for i in range(1,6):\n",
        "  print(score_dict[i][4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vqsXBD7gLQfY"
      },
      "outputs": [],
      "source": [
        "# Cosine\n",
        "import scipy.spatial\n",
        "\n",
        "dl = DataLoader(\"/content/drive/My Drive/Colab Notebooks/SCAN\")\n",
        "file_location = \"/content/drive/My Drive/Colab Notebooks/models/\"\n",
        "hidden_units=200\n",
        "model='LSTM'\n",
        "\n",
        "#train_data, test_data = dl.load_1a()\n",
        "data_dict = dl.load_3()\n",
        "train_data, test_data = data_dict[\"jump\"]\n",
        "\n",
        "train_in = Input(\"train_input\")\n",
        "train_out = Output(\"train_output\")\n",
        "\n",
        "test_in = Input(\"test_input\")\n",
        "test_out = Output(\"test_output\")\n",
        "\n",
        "\n",
        "for datapoint in train_data:\n",
        "    train_in.addSentence(datapoint[0])\n",
        "    train_out.addSentence(datapoint[1])\n",
        "\n",
        "for datapoint in test_data:\n",
        "    test_in.addSentence(datapoint[0])\n",
        "    test_out.addSentence(datapoint[1])\n",
        "\n",
        "encoder, decoder = load_models(train_in.n_words, train_out.n_words, hidden_units, 2, model, 0.5, False, file_location, \"3rd_LSTM_200_dim_2_layer_no_att_0.5_drop_jump1\") #LSTM_no_att_drop_rerun1\n",
        "hit, hit_idx, miss, miss_idx, hidden_state_vectors = evaluateItersAlt(train_data, encoder, decoder, train_in, train_out, vectors=True)\n",
        "#hit, hit_idx, miss, miss_idx = evaluateIters(train_data, encoder, decoder, train_in, train_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cwIvAVB0W1oc"
      },
      "outputs": [],
      "source": [
        "_, run_vec = evaluateAlt(encoder, decoder, \"jump twice\", train_in, train_out, 1)\n",
        "\n",
        "run_vec = run_vec.cpu()\n",
        "\n",
        "testing = []\n",
        "new_hidden_state_vectors = np.asarray(hidden_state_vectors)\n",
        "for x in new_hidden_state_vectors:\n",
        "  testing.append((x[0], 1-scipy.spatial.distance.cosine(run_vec[1], x[1][1].cpu())))\n",
        "\n",
        "dtype = [('command', 'S100'), ('cosine', float)]\n",
        "testing = np.array(testing, dtype=dtype)\n",
        "\n",
        "testing2 = np.sort(testing, order=\"cosine\")\n",
        "\n",
        "#vfunc = np.vectorize(myCosine)\n",
        "#new_hidden_state_vectors = vfunc(hidden_state_vectors, run_vec)\n",
        "#testing = np.sort(new_hidden_state_vectors, axis=2) #?\n",
        "testing2[len(testing2)-6:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "S2EL2eBBx_Tg"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "# count up the command and action lengths\n",
        "\n",
        "train_data, test_data = dl.load_2()\n",
        "\n",
        "train_in = Input(\"train_input\")\n",
        "train_out = Output(\"train_output\")\n",
        "\n",
        "for datapoint in train_data:\n",
        "    train_in.addSentence(datapoint[0])\n",
        "    train_out.addSentence(datapoint[1])\n",
        "\n",
        "test_in = Input(\"test_input\")\n",
        "test_out = Output(\"test_output\")\n",
        "\n",
        "for datapoint in test_data:\n",
        "    test_in.addSentence(datapoint[0])\n",
        "    test_out.addSentence(datapoint[1])\n",
        "\n",
        "file_location = \"/content/drive/My Drive/Colab Notebooks/models/\"\n",
        "hidden_units=200\n",
        "model='LSTM'\n",
        "model_name = \"2nd_LSTM_200_dim_2_layer_no_att_drop\"\n",
        "\n",
        "action_length_dict = defaultdict(int)\n",
        "command_length_dict = defaultdict(int)\n",
        "\n",
        "for point in test_data:\n",
        "  command_length=len(point[0].split())\n",
        "  action_length=len(point[1].split())\n",
        "  action_length_dict[action_length] +=1\n",
        "  command_length_dict[command_length] +=1\n",
        "\n",
        "\n",
        "total_miss_command_length_dict = defaultdict(list)\n",
        "total_miss_action_length_dict = defaultdict(list)\n",
        "\n",
        "for i in range(1, 6):\n",
        "  encoder, decoder = load_models(train_in.n_words, train_out.n_words, hidden_units, 2, model, 0.5, False, file_location, model_name+str(i))\n",
        "  hit, hit_idx, miss, miss_idx = evaluateItersAlt(test_data, encoder, decoder, train_in, train_out)\n",
        "\n",
        "  miss_command_length_dict = defaultdict(int)\n",
        "  miss_action_length_dict = defaultdict(int)\n",
        "\n",
        "  for idx in miss_idx:\n",
        "    command_length=len(test_data[idx][0].split())\n",
        "    action_length=len(test_data[idx][1].split())\n",
        "    miss_action_length_dict[action_length] += 1\n",
        "    miss_command_length_dict[command_length] += 1\n",
        "\n",
        "  for key in miss_action_length_dict:\n",
        "    action_length_acc = 1-miss_action_length_dict[key]/action_length_dict[key]\n",
        "    total_miss_action_length_dict[key].append(action_length_acc)\n",
        "\n",
        "  for key in miss_command_length_dict:\n",
        "    command_length_acc = 1-miss_command_length_dict[key]/command_length_dict[key]\n",
        "    total_miss_command_length_dict[key].append(command_length_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Rtg3BEtbeCRT"
      },
      "outputs": [],
      "source": [
        "command_testing = total_miss_command_length_dict\n",
        "action_testing = total_miss_action_length_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "An1eUij2lhhd"
      },
      "outputs": [],
      "source": [
        "#ACCURACY\n",
        "acc=1-miss/test_data.shape[0]\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HexSo94Alhhn"
      },
      "outputs": [],
      "source": [
        "#SOME RANDOM PRINTING/EVALUATION VALIDATION\n",
        "idx = 2213\n",
        "print(test_data[idx][0])\n",
        "print(len(evaluate(encoder, decoder, test_data[idx][0], train_in, train_out)))\n",
        "print(\" \".join(evaluate(encoder, decoder, test_data[idx][0], train_in, train_out)))\n",
        "print(len(test_data[idx][1].split()))\n",
        "print(test_data[idx][1])"
      ]
    }
  ]
}