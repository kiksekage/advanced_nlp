{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "2Hs18UD0-NrX",
    "outputId": "748d5e5d-86a2-4f6c-eb3e-540b99281343"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rK22Klmm_crY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, filepath):\n",
    "        cwd = os.getcwd()\n",
    "        self.basepath = filepath\n",
    "        try:\n",
    "            os.stat(self.basepath+\"/add_prim_split\")\n",
    "            os.stat(self.basepath+\"/few_shot_split\")\n",
    "            os.stat(self.basepath+\"/filler_split\")\n",
    "            os.stat(self.basepath+\"/length_split\")\n",
    "            os.stat(self.basepath+\"/simple_split\")\n",
    "            os.stat(self.basepath+\"/template_split\")\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Path \"+filepath+\" doesnt seem to contain the required folders.\")\n",
    "\n",
    "    def load_1a(self):\n",
    "        train = self.file_loader(\"/simple_split/tasks_train_simple.txt\")\n",
    "        test = self.file_loader(\"/simple_split/tasks_test_simple.txt\")\n",
    "\n",
    "        return (np.asarray(train), np.asarray(test))\n",
    "\n",
    "    def load_1b(self):\n",
    "        percentile_dict = {}\n",
    "        splits = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\"]\n",
    "\n",
    "        for percentile in splits:\n",
    "            train = self.file_loader(\"/simple_split/size_variations/tasks_train_simple_p{}.txt\".format(percentile))\n",
    "            test = self.file_loader(\"/simple_split/size_variations/tasks_test_simple_p{}.txt\".format(percentile))\n",
    "            \n",
    "            percentile_dict[percentile] = (np.asarray(train), np.asarray(test))\n",
    "            \n",
    "        return percentile_dict\n",
    "\n",
    "    def load_2(self):\n",
    "        train = self.file_loader(\"/length_split/tasks_train_length.txt\")\n",
    "        test = self.file_loader(\"/length_split/tasks_test_length.txt\")\n",
    "\n",
    "        return (np.asarray(train), np.asarray(test))\n",
    "\n",
    "    def load_3(self):\n",
    "        \"\"\"\n",
    "        loads the datasets for both parts of the experiment\n",
    "        the first part where both primitives appear without compositional commands\n",
    "        the second part where 'jump' primitive appears in\n",
    "        compositional commands of varying lengths\n",
    "        returns a dictionary of pairs all possible train/test sets\n",
    "        \"\"\"\n",
    "        data_dict = {}\n",
    "        nums = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]\n",
    "        reps = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "\n",
    "        train = self.file_loader(\"/add_prim_split/tasks_train_addprim_jump.txt\")\n",
    "        test = self.file_loader(\"/add_prim_split/tasks_test_addprim_jump.txt\")\n",
    "        data_dict['jump'] = (np.asarray(train), np.asarray(test))\n",
    "\n",
    "        train = self.file_loader(\"/add_prim_split/tasks_train_addprim_turn_left.txt\")\n",
    "        test = self.file_loader(\"/add_prim_split/tasks_test_addprim_turn_left.txt\")\n",
    "        data_dict['lturn'] = (np.asarray(train), np.asarray(test))\n",
    "        \n",
    "        for num in nums:\n",
    "            for rep in reps:\n",
    "                train = self.file_loader(\"/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num{}_rep{}.txt\".format(num, rep))\n",
    "                test = self.file_loader(\"/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num{}_rep{}.txt\".format(num, rep))\n",
    "                \n",
    "                data_dict['jump_num{}_rep{}'.format(num, rep)] = (np.asarray(train), np.asarray(test))\n",
    "            \n",
    "        return data_dict\n",
    "\n",
    "    def file_loader(self, path):\n",
    "        sent_list = []\n",
    "        with open(self.basepath+path, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        sent_list.append(line_splitter(line))\n",
    "        return sent_list\n",
    "\n",
    "    \n",
    "def line_splitter(sentence):\n",
    "    sent_list = sentence.split(\"OUT: \")\n",
    "    sent_list[0] = sent_list[0].strip(\"IN: \")\n",
    "    sent_list[1] = sent_list[1].strip(\"\\n\")\n",
    "\n",
    "    return sent_list\n",
    "\n",
    "# examples:\n",
    "# 1a :\n",
    "#   train, test = dl.load_1a()\n",
    "#   train[0][0] first train sentence, \"IN\"\n",
    "#   train[0][1] first train sentence, \"OUT\"\n",
    "# 1b :\n",
    "#   dict = dl.load_1b()\n",
    "#   train, test = dict[\"1\"] extract the 1 percentile sentences out, split into train and test\n",
    "#   train[0][0] first train sentence, \"OUT\"\n",
    "#   train[0][1] first train sentence, \"OUT\"\n",
    "#\n",
    "# all returns are numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6j2NebXKH_zk"
   },
   "outputs": [],
   "source": [
    "#from data_loader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Input:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        #self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.index2word = {}\n",
    "        #self.n_words = 2  # Count SOS and EOS\n",
    "        self.n_words = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "class Output:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        #self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.index2word = {}\n",
    "        #self.n_words = 2  # Count SOS and EOS\n",
    "        self.n_words = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "        \n",
    "def get_embedding(word, lookup_dict, embeds):\n",
    "    tensor = torch.tensor([lookup_dict[word]], dtype=torch.long)\n",
    "    return embeds(tensor)\n",
    "\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair, input_lang, output_lang):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    output_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hSbsv0iCHyrd",
    "outputId": "7b588907-578f-4fd1-a86c-53f737169b1a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "#from data_loader import *\n",
    "#from embeddings import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size,num_layers=2) # num_layers=2,\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "\n",
    "        hidden1, hidden2 = hidden\n",
    "        output, (hidden1, hidden2) = self.lstm(output, (hidden1, hidden2))\n",
    "        return output, (hidden1, hidden2)\n",
    "\n",
    "    def initHidden(self):\n",
    "        hidden = torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "        nn.init.xavier_uniform_(hidden, gain=nn.init.calculate_gain('relu'))\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size,num_layers=2) #num_layers=2\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        hidden1, hidden2 = hidden\n",
    "        output, (hidden1, hidden2) = self.lstm(output, (hidden1, hidden2))\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, (hidden1, hidden2)\n",
    "\n",
    "    def initHidden(self):\n",
    "        hidden = torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "        nn.init.xavier_uniform_(hidden, gain=nn.init.calculate_gain('relu'))\n",
    "        return hidden\n",
    "\n",
    "\n",
    "def train(input_tensor, output_tensor, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion, max_length=100):\n",
    "    encoder_hidden1 = encoder.initHidden()\n",
    "    encoder_hidden2 = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    output_length = output_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei], (encoder_hidden1, encoder_hidden2))\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_hidden1 = encoder_hidden1\n",
    "    decoder_hidden2 = encoder_hidden2\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(output_length):\n",
    "            decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2))\n",
    "            loss += criterion(decoder_output, output_tensor[di])\n",
    "            decoder_input = output_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        for di in range(output_length):\n",
    "            decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2))\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            loss += criterion(decoder_output, output_tensor[di])\n",
    "\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "    clip_grad_norm_(decoder.parameters(),5.0)\n",
    "    clip_grad_norm_(encoder.parameters(),5.0)\n",
    "    \n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / output_length\n",
    "\n",
    "    \n",
    "def trainIters(encoder, decoder, train_data, input_lang, output_lang, learning_rate=0.001):\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    loss_plot = []\n",
    "    losses = []\n",
    "    print_loss_total = 0\n",
    "    print(train_data.shape[0])\n",
    "    k = 0\n",
    "    perc_complete = 1\n",
    "    #for iter in range(train_data.shape[0]):\n",
    "    iterations = 20000\n",
    "    while k <= iterations:\n",
    "        training_pair = tensorsFromPair(random.choice(train_data), input_lang, output_lang)\n",
    "        input_tensor = training_pair[0]\n",
    "        output_tensor = training_pair[1]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            input_tensor = input_tensor.cuda()\n",
    "            output_tensor = output_tensor.cuda()\n",
    "\n",
    "        loss = train(input_tensor, output_tensor, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion)\n",
    "        losses.append(loss)\n",
    "        print_loss_total += loss\n",
    "        #print(lol)\n",
    "        k += 1\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "        ave_over = 100\n",
    "        if k % ave_over == 0:\n",
    "            print_loss_avg = print_loss_total / ave_over\n",
    "            #print(print_loss_avg)\n",
    "            loss_plot.append(print_loss_avg)\n",
    "            print_loss_total = 0\n",
    "        \n",
    "        if k % (iterations/100) == 0:\n",
    "            print(str(perc_complete) + \"% of training completed!\")\n",
    "            perc_complete += 1\n",
    "    plt.plot(loss_plot)\n",
    "    return losses\n",
    "\n",
    "basepath = os.getcwd() + \"/SCAN\"\n",
    "dl = DataLoader(basepath)\n",
    "train_data, test_data = dl.load_1a()\n",
    "\n",
    "train_in = Input(\"train_input\")\n",
    "train_out = Output(\"train_output\")\n",
    "\n",
    "test_in = Input(\"test_input\")\n",
    "test_out = Output(\"test_output\")\n",
    "\n",
    "for datapoint in train_data:\n",
    "        train_in.addSentence(datapoint[0])\n",
    "        train_out.addSentence(datapoint[1])\n",
    "\n",
    "for datapoint in test_data:\n",
    "        test_in.addSentence(datapoint[0])\n",
    "        test_out.addSentence(datapoint[1])\n",
    "\n",
    "encoder = Encoder(train_in.n_words, 200)\n",
    "decoder = Decoder(200,train_out.n_words)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   encoder.cuda()\n",
    "   decoder.cuda()\n",
    "\n",
    "start = time.time()\n",
    "losses = trainIters(encoder, decoder, train_data, train_in, train_out)\n",
    "stop = time.time()\n",
    "#print(losses)\n",
    "print(stop-start)\n",
    "#plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang, max_length=100):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden1 = encoder.initHidden()\n",
    "        encoder_hidden2 = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei],\n",
    "                                                     (encoder_hidden1, encoder_hidden2))\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "        decoder_hidden1 = encoder_hidden1\n",
    "        decoder_hidden2 = encoder_hidden2\n",
    "\n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2))\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                #decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words\n",
    "\n",
    "def evaluateIters(test_data, encoder, decoder, lang_in, lang_out):\n",
    "    miss = 0\n",
    "    iters = 0\n",
    "\n",
    "    for test_point in test_data:\n",
    "        pred = evaluate(encoder, decoder, test_point[0], lang_in, lang_out)\n",
    "        pred = \" \".join(pred)\n",
    "        if pred != test_point[1]:\n",
    "            miss += 1\n",
    "        iters += 1\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print(iters)\n",
    "            print(miss)\n",
    "\n",
    "    return miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss = evaluateIters(test_data, encoder, decoder, train_in, train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1-(miss/test_data.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "initial_train_attempt_lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
